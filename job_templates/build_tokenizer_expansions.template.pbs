#!/bin/bash

#PBS -N build_tokenizer
#PBS -q YOUR_QUEUE_NAME
#PBS -l select=1:ncpus=YOUR_NCPUS:mem=YOUR_MEMORY
#PBS -l walltime=YOUR_WALLTIME
#PBS -j oe
#PBS -o /path/to/your/logs/

# ============================================================================
# PBS Job Script for Building Tokenizer Expansions - TEMPLATE
# ============================================================================
#
# This is a TEMPLATE file. Copy to jobs/ and customize:
#   1. Replace YOUR_QUEUE_NAME with your cluster queue
#   2. Replace /path/to/your/logs/ with your log directory
#   3. Replace YOUR_NCPUS (typically 4)
#   4. Replace YOUR_MEMORY (typically 32GB)
#   5. Replace YOUR_WALLTIME (e.g., 08:00:00)
#
# Usage:
#   cp job_templates/build_tokenizer_expansions.template.pbs jobs/build_tokenizer_expansions.pbs
#   vim jobs/build_tokenizer_expansions.pbs  # Customize
#   qsub -v TOKENIZER_NAME=gpt2 jobs/build_tokenizer_expansions.pbs
#   qsub -v TOKENIZER_NAME=google/gemma-3-1b-pt jobs/build_tokenizer_expansions.pbs
#
# ============================================================================

# Load environment variables
cd /path/to/your/project
source .env

# Activate conda environment
source env/bin/activate

# Check for required parameter
if [ -z "$TOKENIZER_NAME" ]; then
    echo "Error: TOKENIZER_NAME not set"
    echo "Usage: qsub -v TOKENIZER_NAME=gpt2 jobs/build_tokenizer_expansions.pbs"
    echo "   or: qsub -v TOKENIZER_NAME=google/gemma-3-1b-pt jobs/build_tokenizer_expansions.pbs"
    exit 1
fi

echo "================================================================"
echo "Building Tokenizer Expansions"
echo "================================================================"
echo "Tokenizer: $TOKENIZER_NAME"
echo "Start time: $(date)"
echo "================================================================"

# Build the expansions
python3 << EOF
import sys
import os
sys.path.insert(0, 'src')

from transformers import AutoTokenizer
from tokenization import StochastokProcessor

tokenizer_name = "$TOKENIZER_NAME"
print(f"Loading tokenizer: {tokenizer_name}")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
print(f"✓ Loaded tokenizer: {type(tokenizer).__name__}")
print(f"  Vocab size: {tokenizer.vocab_size}")
print()

print("Building tokenizer expansions...")
print("(This may take several hours for large vocabularies)")
processor = StochastokProcessor(tokenizer, expand_prop=0.1)
print()

print("✓ Tokenizer expansions built successfully")
print(f"  Saved to: {processor.expansions_path}")
EOF

echo "================================================================"
echo "Complete!"
echo "End time: $(date)"
echo "================================================================"
