#!/bin/bash

#PBS -N gemma3_dryrun_vanilla
#PBS -l select=1:ngpus=2
#PBS -l walltime=01:00:00
#PBS -q AISG_debug
#PBS -j oe
#PBS -o /scratch_aisg/SPEC-SF-AISG/railey/logs/

# ============================================================================
# PBS Job Script for Dry Run CPT with Vanilla Tokenization
# ============================================================================
#
# This is a dry run test to validate training with vanilla tokenized data.
#
# Usage:
#   qsub jobs/run_cpt_dryrun_vanilla.pbs
#
# ============================================================================

set -euo pipefail

# ============================================================================
# Setup Environment
# ============================================================================

export JOB_WORK_DIR=${PBS_O_WORKDIR}
export JOB_ID=${PBS_JOBID}
export JOB_NAME=${PBS_JOBNAME}

cd ${JOB_WORK_DIR}

# Activate Python environment
if [ -f "${JOB_WORK_DIR}/env/bin/activate" ]; then
    source "${JOB_WORK_DIR}/env/bin/activate"
else
    echo "Warning: Python environment not found at ${JOB_WORK_DIR}/env/bin/activate"
fi

# Load environment variables
if [ -f "${JOB_WORK_DIR}/.env" ]; then
    echo "Loading environment from .env file..."
    source "${JOB_WORK_DIR}/.env"
else
    echo "Error: .env file not found at ${JOB_WORK_DIR}/.env"
    exit 1
fi

# ============================================================================
# Container Configuration
# ============================================================================

export CONTAINER_NAME="nemo_framework"

if ! enroot list | grep -q "^${CONTAINER_NAME}$"; then
    echo "Error: Container ${CONTAINER_NAME} not found"
    echo "Please run setup_enroot.sh first"
    exit 1
fi

# ============================================================================
# Verify Data Availability
# ============================================================================

echo "============================================================================"
echo "Checking Data Availability"
echo "============================================================================"

DATA_DIR="${JOB_WORK_DIR}/data/processed/google-gemma-3-1b-pt"

if [ ! -d "${DATA_DIR}" ]; then
    echo "Error: Data directory not found: ${DATA_DIR}"
    exit 1
fi

# Count available chunks
CHUNK_COUNT=$(ls ${DATA_DIR}/chunk_*.bin 2>/dev/null | wc -l)
echo "Found ${CHUNK_COUNT} preprocessed chunks in ${DATA_DIR}"

if [ ${CHUNK_COUNT} -eq 0 ]; then
    echo "Error: No preprocessed data chunks found"
    exit 1
fi

# List first few chunks
echo ""
echo "Sample chunks:"
ls -lh ${DATA_DIR}/chunk_000{1,2,3}.{bin,idx} 2>/dev/null || true
echo ""

# ============================================================================
# Directory Setup
# ============================================================================

export LOG_DIR="/scratch_aisg/SPEC-SF-AISG/railey/logs/training/${JOB_ID}"
export CKPT_DIR="/scratch_aisg/SPEC-SF-AISG/railey/logs/checkpoints/gemma3-dryrun-vanilla-${JOB_ID}"

mkdir -p ${LOG_DIR}
mkdir -p ${CKPT_DIR}

# ============================================================================
# Dry Run Configuration (Very small values for quick validation)
# ============================================================================

# Build data paths for first 3 chunks only (for quick test)
# Note: Paths should NOT include _text_document suffix (script adds it)
export DATA_PATHS=""
for i in {1..3}; do
    chunk_num=$(printf "%04d" $i)
    DATA_PATHS="${DATA_PATHS} /workspace/data/processed/google-gemma-3-1b-pt/chunk_${chunk_num}"
done

export SEQ_LENGTH=512
export MAX_STEPS=10
export GBS=4
export MBS=1
export DEVICES=1
export LR=1e-4
export MIN_LR=1e-5
export WARMUP_STEPS=5
export CKPT_INTERVAL=10  # Don't save checkpoint for dry run
# Skip resume_from for dry run to avoid HF checkpoint loading issues
# export RESUME_FROM="google/gemma-3-1b-pt"
export WANDB_PROJECT="gemma3-cpt-dryrun"
export WANDB_NAME="vanilla-${JOB_ID}"
export LOG_EVERY_N_STEPS=1
export VAL_CHECK_INTERVAL=5

# ============================================================================
# Build Python Command
# ============================================================================

PYTHON_CMD="python /workspace/training/nemo/run_cpt.py \
    --data-path ${DATA_PATHS} \
    --seq-length ${SEQ_LENGTH} \
    --max-steps ${MAX_STEPS} \
    --global-batch-size ${GBS} \
    --micro-batch-size ${MBS} \
    --devices ${DEVICES} \
    --lr ${LR} \
    --min-lr ${MIN_LR} \
    --warmup-steps ${WARMUP_STEPS} \
    --checkpoint-dir ${CKPT_DIR} \
    --checkpoint-interval ${CKPT_INTERVAL} \
    --wandb-project ${WANDB_PROJECT} \
    --wandb-name ${WANDB_NAME} \
    --log-dir ${LOG_DIR} \
    --log-every-n-steps ${LOG_EVERY_N_STEPS} \
    --val-check-interval ${VAL_CHECK_INTERVAL}"

# Note: Removed --resume-from for dry run to avoid HF checkpoint loading issues

# ============================================================================
# Print Configuration
# ============================================================================

echo "============================================================================"
echo "Dry Run Configuration - VANILLA Tokenization"
echo "============================================================================"
echo "Job ID: ${JOB_ID}"
echo "Container: ${CONTAINER_NAME}"
echo ""
echo "Data Configuration:"
echo "  Chunks: 3 (chunk_0001 to chunk_0003)"
echo "  Tokenization: Vanilla (standard)"
echo "  Data paths (without .bin/.idx suffix): ${DATA_PATHS}"
echo ""
echo "Training Configuration:"
echo "  Max Steps: ${MAX_STEPS}"
echo "  Sequence Length: ${SEQ_LENGTH}"
echo "  Global Batch Size: ${GBS}"
echo "  Micro Batch Size: ${MBS}"
echo "  Devices: ${DEVICES}"
echo ""
echo "Output:"
echo "  Logs: ${LOG_DIR}"
echo "  Checkpoints: ${CKPT_DIR}"
echo "============================================================================"
echo ""

# ============================================================================
# Build Enroot Command
# ============================================================================

ENROOT_MOUNTS="--mount ${JOB_WORK_DIR}:/workspace"

if [ -n "${BIND_MOUNTS:-}" ]; then
    IFS=',' read -ra MOUNTS <<< "$BIND_MOUNTS"
    for mount in "${MOUNTS[@]}"; do
        ENROOT_MOUNTS="${ENROOT_MOUNTS} --mount ${mount}"
    done
fi

ENROOT_ENV="-e HF_HOME=${HF_HOME} \
    -e WANDB_API_KEY=${WANDB_API_KEY} \
    -e WANDB_DIR=${WANDB_DIR} \
    -e LOG_DIR=${LOG_DIR} \
    -e CKPT_DIR=${CKPT_DIR} \
    -e NVIDIA_DISABLE_REQUIRE=1"

# ============================================================================
# Launch Dry Run
# ============================================================================

echo "Launching dry run with vanilla tokenization..."
echo ""

enroot start --rw \
    ${ENROOT_ENV} \
    ${ENROOT_MOUNTS} \
    ${CONTAINER_NAME} \
    bash -c "${PYTHON_CMD}" 2>&1 | tee ${LOG_DIR}/dryrun_vanilla.log

EXIT_CODE=$?

echo ""
echo "============================================================================"
echo "Dry Run Completed - Exit Code: ${EXIT_CODE}"
echo "============================================================================"
echo "Check logs at: ${LOG_DIR}"
echo "============================================================================"

exit ${EXIT_CODE}
